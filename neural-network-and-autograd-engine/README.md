# Micrograd: A Minimal Autograd Engine and Neural Network Library

This repository follows Andrej Karpathy's guide to building **Micrograd**, a basic autograd engine and neural network library implemented from scratch in Python. 

[Link to Karpathy's Tutorial](https://youtu.be/VMj-3S1tku0?si=KbHUGX7qJAhWWZU9)
## Who is Andrej Karpathy?

Andrej Karpathy is a renowned AI researcher, educator, and engineer who has contributed significantly to the field of deep learning. Formerly the Director of AI at Tesla and a key figure at OpenAI, Karpathy is known for his ability to simplify complex AI concepts for learners worldwide.

## What is This Project?

This project is my implementation of **Micrograd**, based on Karpathy's guide. It includes:

- A minimal **autograd engine** that enables automatic differentiation for scalar-valued computations.
- A basic **neural network library** to build and train small neural networks.

## Why is This Useful?

Micrograd is a fantastic educational tool for understanding the foundational mechanics of automatic differentiation and neural networks. While it is not intended for production use, it provides valuable insights into:

- How modern machine learning libraries like PyTorch or TensorFlow implement backpropagation.
- The inner workings of gradient computation and optimization.

If you're curious about how deep learning frameworks operate under the hood, this project is a great place to start!

